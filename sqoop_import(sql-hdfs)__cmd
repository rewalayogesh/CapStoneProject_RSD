[cloudera@quickstart ~]$ sqoop
Warning: /usr/lib/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.
Try 'sqoop help' for usage.
[cloudera@quickstart ~]$ export ACCUMULO_HOME=''
[cloudera@quickstart ~]$ sqoop
Warning: /usr/lib/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.
Try 'sqoop help' for usage.
[cloudera@quickstart ~]$ nano ~/.bashrc
[cloudera@quickstart ~]$ nano ~/.bashrc
[cloudera@quickstart ~]$ source ~/.bashrc
[cloudera@quickstart ~]$ echo $ACCUMULO_HOME

[cloudera@quickstart ~]$ sqoop
Warning: /usr/lib/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.
Try 'sqoop help' for usage.
[cloudera@quickstart ~]$ sqoop import\
> --connect jdbc:mysql://localhost/Capstone\
> --username=root\
> --password=cloudera\
> --table=City_data\
> --target-dir /queryresults
Warning: /usr/lib/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.
No such sqoop tool: import--connect. See 'sqoop help'.
[cloudera@quickstart ~]$ sqoop import --connect jdbc:mysql://localhost/Capstone --username=root --password=cloudera --table=City_data --target-dir /queryresults
Warning: /usr/lib/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.
23/09/20 00:51:48 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.13.0
23/09/20 00:51:48 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.
23/09/20 00:51:49 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.
23/09/20 00:51:49 INFO tool.CodeGenTool: Beginning code generation
23/09/20 00:51:50 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `City_data` AS t LIMIT 1
23/09/20 00:51:50 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `City_data` AS t LIMIT 1
23/09/20 00:51:50 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /usr/lib/hadoop-mapreduce
Note: /tmp/sqoop-cloudera/compile/e3196011362afdb2e7812a28dbffcc5a/City_data.java uses or overrides a deprecated API.
Note: Recompile with -Xlint:deprecation for details.
23/09/20 00:51:57 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-cloudera/compile/e3196011362afdb2e7812a28dbffcc5a/City_data.jar
23/09/20 00:51:57 WARN manager.MySQLManager: It looks like you are importing from mysql.
23/09/20 00:51:57 WARN manager.MySQLManager: This transfer can be faster! Use the --direct
23/09/20 00:51:57 WARN manager.MySQLManager: option to exercise a MySQL-specific fast path.
23/09/20 00:51:57 INFO manager.MySQLManager: Setting zero DATETIME behavior to convertToNull (mysql)
23/09/20 00:51:57 WARN manager.CatalogQueryManager: The table City_data contains a multi-column primary key. Sqoop will default to the column Date only for this job.
23/09/20 00:51:57 WARN manager.CatalogQueryManager: The table City_data contains a multi-column primary key. Sqoop will default to the column Date only for this job.
23/09/20 00:51:57 INFO mapreduce.ImportJobBase: Beginning import of City_data
23/09/20 00:51:57 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address
23/09/20 00:51:59 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar
23/09/20 00:52:02 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps
23/09/20 00:52:02 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032
23/09/20 00:52:07 WARN hdfs.DFSClient: Caught exception 
java.lang.InterruptedException
	at java.lang.Object.wait(Native Method)
	at java.lang.Thread.join(Thread.java:1281)
	at java.lang.Thread.join(Thread.java:1355)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFSOutputStream.java:967)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.endBlock(DFSOutputStream.java:705)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:894)
23/09/20 00:52:07 WARN hdfs.DFSClient: Caught exception 
java.lang.InterruptedException
	at java.lang.Object.wait(Native Method)
	at java.lang.Thread.join(Thread.java:1281)
	at java.lang.Thread.join(Thread.java:1355)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFSOutputStream.java:967)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.endBlock(DFSOutputStream.java:705)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:894)
23/09/20 00:52:08 WARN hdfs.DFSClient: Caught exception 
java.lang.InterruptedException
	at java.lang.Object.wait(Native Method)
	at java.lang.Thread.join(Thread.java:1281)
	at java.lang.Thread.join(Thread.java:1355)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFSOutputStream.java:967)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.endBlock(DFSOutputStream.java:705)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:894)
23/09/20 00:52:08 WARN hdfs.DFSClient: Caught exception 
java.lang.InterruptedException
	at java.lang.Object.wait(Native Method)
	at java.lang.Thread.join(Thread.java:1281)
	at java.lang.Thread.join(Thread.java:1355)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFSOutputStream.java:967)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.endBlock(DFSOutputStream.java:705)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:894)
23/09/20 00:52:08 WARN hdfs.DFSClient: Caught exception 
java.lang.InterruptedException
	at java.lang.Object.wait(Native Method)
	at java.lang.Thread.join(Thread.java:1281)
	at java.lang.Thread.join(Thread.java:1355)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFSOutputStream.java:967)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.endBlock(DFSOutputStream.java:705)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:894)
23/09/20 00:52:08 WARN hdfs.DFSClient: Caught exception 
java.lang.InterruptedException
	at java.lang.Object.wait(Native Method)
	at java.lang.Thread.join(Thread.java:1281)
	at java.lang.Thread.join(Thread.java:1355)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFSOutputStream.java:967)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.endBlock(DFSOutputStream.java:705)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:894)
23/09/20 00:52:12 INFO db.DBInputFormat: Using read commited transaction isolation
23/09/20 00:52:12 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(`Date`), MAX(`Date`) FROM `City_data`
23/09/20 00:52:12 INFO db.IntegerSplitter: Split size: 55900800000; Num splits: 4 from: 1291104000000 to: 1514707200000
23/09/20 00:52:12 WARN hdfs.DFSClient: Caught exception 
java.lang.InterruptedException
	at java.lang.Object.wait(Native Method)
	at java.lang.Thread.join(Thread.java:1281)
	at java.lang.Thread.join(Thread.java:1355)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFSOutputStream.java:967)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.endBlock(DFSOutputStream.java:705)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:894)
23/09/20 00:52:12 WARN hdfs.DFSClient: Caught exception 
java.lang.InterruptedException
	at java.lang.Object.wait(Native Method)
	at java.lang.Thread.join(Thread.java:1281)
	at java.lang.Thread.join(Thread.java:1355)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFSOutputStream.java:967)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.endBlock(DFSOutputStream.java:705)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:894)
23/09/20 00:52:12 INFO mapreduce.JobSubmitter: number of splits:4
23/09/20 00:52:13 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1695115881461_0001
23/09/20 00:52:15 INFO impl.YarnClientImpl: Submitted application application_1695115881461_0001
23/09/20 00:52:16 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1695115881461_0001/
23/09/20 00:52:16 INFO mapreduce.Job: Running job: job_1695115881461_0001
23/09/20 00:52:52 INFO mapreduce.Job: Job job_1695115881461_0001 running in uber mode : false
23/09/20 00:52:52 INFO mapreduce.Job:  map 0% reduce 0%
23/09/20 00:54:42 INFO mapreduce.Job:  map 100% reduce 0%
23/09/20 00:54:44 INFO mapreduce.Job: Job job_1695115881461_0001 completed successfully
23/09/20 00:54:45 INFO mapreduce.Job: Counters: 30
	File System Counters
		FILE: Number of bytes read=0
		FILE: Number of bytes written=685236
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=497
		HDFS: Number of bytes written=43187049
		HDFS: Number of read operations=16
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=8
	Job Counters 
		Launched map tasks=4
		Other local map tasks=4
		Total time spent by all maps in occupied slots (ms)=426652
		Total time spent by all reduces in occupied slots (ms)=0
		Total time spent by all map tasks (ms)=426652
		Total vcore-milliseconds taken by all map tasks=426652
		Total megabyte-milliseconds taken by all map tasks=436891648
	Map-Reduce Framework
		Map input records=919776
		Map output records=919776
		Input split bytes=497
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=3794
		CPU time spent (ms)=33420
		Physical memory (bytes) snapshot=486432768
		Virtual memory (bytes) snapshot=6045532160
		Total committed heap usage (bytes)=243007488
	File Input Format Counters 
		Bytes Read=0
	File Output Format Counters 
		Bytes Written=43187049
23/09/20 00:54:45 INFO mapreduce.ImportJobBase: Transferred 41.1864 MB in 163.0687 seconds (258.6323 KB/sec)
23/09/20 00:54:45 INFO mapreduce.ImportJobBase: Retrieved 919776 records.
[cloudera@quickstart ~]$ sqoop import --connect jdbc:mysql://localhost/Capstone --username=root --password=cloudera --table=cities_crosswalk --target-dir /queryresults
Warning: /usr/lib/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.
23/09/20 00:55:42 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.13.0
23/09/20 00:55:42 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.
23/09/20 00:55:43 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.
23/09/20 00:55:43 INFO tool.CodeGenTool: Beginning code generation
23/09/20 00:55:45 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `cities_crosswalk` AS t LIMIT 1
23/09/20 00:55:45 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `cities_crosswalk` AS t LIMIT 1
23/09/20 00:55:45 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /usr/lib/hadoop-mapreduce
Note: /tmp/sqoop-cloudera/compile/842ca18f5745e7f93134552c65b8e545/cities_crosswalk.java uses or overrides a deprecated API.
Note: Recompile with -Xlint:deprecation for details.
23/09/20 00:55:51 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-cloudera/compile/842ca18f5745e7f93134552c65b8e545/cities_crosswalk.jar
23/09/20 00:55:51 WARN manager.MySQLManager: It looks like you are importing from mysql.
23/09/20 00:55:51 WARN manager.MySQLManager: This transfer can be faster! Use the --direct
23/09/20 00:55:51 WARN manager.MySQLManager: option to exercise a MySQL-specific fast path.
23/09/20 00:55:51 INFO manager.MySQLManager: Setting zero DATETIME behavior to convertToNull (mysql)
23/09/20 00:55:51 INFO mapreduce.ImportJobBase: Beginning import of cities_crosswalk
23/09/20 00:55:51 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address
23/09/20 00:55:53 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar
23/09/20 00:55:56 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps
23/09/20 00:55:56 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032
23/09/20 00:55:57 WARN security.UserGroupInformation: PriviledgedActionException as:cloudera (auth:SIMPLE) cause:org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory hdfs://quickstart.cloudera:8020/queryresults already exists
23/09/20 00:55:57 ERROR tool.ImportTool: Import failed: org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory hdfs://quickstart.cloudera:8020/queryresults already exists
	at org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:146)
	at org.apache.hadoop.mapreduce.JobSubmitter.checkSpecs(JobSubmitter.java:270)
	at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:143)
	at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1307)
	at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1304)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1917)
	at org.apache.hadoop.mapreduce.Job.submit(Job.java:1304)
	at org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:1325)
	at org.apache.sqoop.mapreduce.ImportJobBase.doSubmitJob(ImportJobBase.java:203)
	at org.apache.sqoop.mapreduce.ImportJobBase.runJob(ImportJobBase.java:176)
	at org.apache.sqoop.mapreduce.ImportJobBase.runImport(ImportJobBase.java:273)
	at org.apache.sqoop.manager.SqlManager.importTable(SqlManager.java:692)
	at org.apache.sqoop.manager.MySQLManager.importTable(MySQLManager.java:127)
	at org.apache.sqoop.tool.ImportTool.importTable(ImportTool.java:513)
	at org.apache.sqoop.tool.ImportTool.run(ImportTool.java:621)
	at org.apache.sqoop.Sqoop.run(Sqoop.java:147)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)
	at org.apache.sqoop.Sqoop.runSqoop(Sqoop.java:183)
	at org.apache.sqoop.Sqoop.runTool(Sqoop.java:234)
	at org.apache.sqoop.Sqoop.runTool(Sqoop.java:243)
	at org.apache.sqoop.Sqoop.main(Sqoop.java:252)

[cloudera@quickstart ~]$ sqoop import --connect jdbc:mysql://localhost/Capstone --username=root --password=cloudera --table=cities_crosswalk --target-dir /queryresults1 
Warning: /usr/lib/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.
23/09/20 00:56:21 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.13.0
23/09/20 00:56:21 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.
23/09/20 00:56:22 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.
23/09/20 00:56:22 INFO tool.CodeGenTool: Beginning code generation
23/09/20 00:56:24 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `cities_crosswalk` AS t LIMIT 1
23/09/20 00:56:24 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `cities_crosswalk` AS t LIMIT 1
23/09/20 00:56:24 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /usr/lib/hadoop-mapreduce
Note: /tmp/sqoop-cloudera/compile/22cbda482d07629fb4faa091ebea7ba9/cities_crosswalk.java uses or overrides a deprecated API.
Note: Recompile with -Xlint:deprecation for details.
23/09/20 00:56:30 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-cloudera/compile/22cbda482d07629fb4faa091ebea7ba9/cities_crosswalk.jar
23/09/20 00:56:30 WARN manager.MySQLManager: It looks like you are importing from mysql.
23/09/20 00:56:30 WARN manager.MySQLManager: This transfer can be faster! Use the --direct
23/09/20 00:56:30 WARN manager.MySQLManager: option to exercise a MySQL-specific fast path.
23/09/20 00:56:30 INFO manager.MySQLManager: Setting zero DATETIME behavior to convertToNull (mysql)
23/09/20 00:56:30 INFO mapreduce.ImportJobBase: Beginning import of cities_crosswalk
23/09/20 00:56:30 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address
23/09/20 00:56:31 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar
23/09/20 00:56:33 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps
23/09/20 00:56:33 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032
23/09/20 00:56:36 WARN hdfs.DFSClient: Caught exception 
java.lang.InterruptedException
	at java.lang.Object.wait(Native Method)
	at java.lang.Thread.join(Thread.java:1281)
	at java.lang.Thread.join(Thread.java:1355)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFSOutputStream.java:967)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.endBlock(DFSOutputStream.java:705)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:894)
23/09/20 00:56:36 WARN hdfs.DFSClient: Caught exception 
java.lang.InterruptedException
	at java.lang.Object.wait(Native Method)
	at java.lang.Thread.join(Thread.java:1281)
	at java.lang.Thread.join(Thread.java:1355)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFSOutputStream.java:967)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.endBlock(DFSOutputStream.java:705)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:894)
23/09/20 00:56:36 WARN hdfs.DFSClient: Caught exception 
java.lang.InterruptedException
	at java.lang.Object.wait(Native Method)
	at java.lang.Thread.join(Thread.java:1281)
	at java.lang.Thread.join(Thread.java:1355)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFSOutputStream.java:967)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.endBlock(DFSOutputStream.java:705)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:894)
23/09/20 00:56:37 WARN hdfs.DFSClient: Caught exception 
java.lang.InterruptedException
	at java.lang.Object.wait(Native Method)
	at java.lang.Thread.join(Thread.java:1281)
	at java.lang.Thread.join(Thread.java:1355)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFSOutputStream.java:967)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.endBlock(DFSOutputStream.java:705)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:894)
23/09/20 00:56:37 WARN hdfs.DFSClient: Caught exception 
java.lang.InterruptedException
	at java.lang.Object.wait(Native Method)
	at java.lang.Thread.join(Thread.java:1281)
	at java.lang.Thread.join(Thread.java:1355)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFSOutputStream.java:967)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.endBlock(DFSOutputStream.java:705)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:894)
23/09/20 00:56:37 WARN hdfs.DFSClient: Caught exception 
java.lang.InterruptedException
	at java.lang.Object.wait(Native Method)
	at java.lang.Thread.join(Thread.java:1281)
	at java.lang.Thread.join(Thread.java:1355)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFSOutputStream.java:967)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.endBlock(DFSOutputStream.java:705)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:894)
23/09/20 00:56:37 WARN hdfs.DFSClient: Caught exception 
java.lang.InterruptedException
	at java.lang.Object.wait(Native Method)
	at java.lang.Thread.join(Thread.java:1281)
	at java.lang.Thread.join(Thread.java:1355)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFSOutputStream.java:967)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.endBlock(DFSOutputStream.java:705)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:894)
23/09/20 00:56:38 WARN hdfs.DFSClient: Caught exception 
java.lang.InterruptedException
	at java.lang.Object.wait(Native Method)
	at java.lang.Thread.join(Thread.java:1281)
	at java.lang.Thread.join(Thread.java:1355)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFSOutputStream.java:967)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.endBlock(DFSOutputStream.java:705)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:894)
23/09/20 00:56:38 WARN hdfs.DFSClient: Caught exception 
java.lang.InterruptedException
	at java.lang.Object.wait(Native Method)
	at java.lang.Thread.join(Thread.java:1281)
	at java.lang.Thread.join(Thread.java:1355)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFSOutputStream.java:967)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.endBlock(DFSOutputStream.java:705)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:894)
23/09/20 00:56:39 WARN hdfs.DFSClient: Caught exception 
java.lang.InterruptedException
	at java.lang.Object.wait(Native Method)
	at java.lang.Thread.join(Thread.java:1281)
	at java.lang.Thread.join(Thread.java:1355)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFSOutputStream.java:967)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.endBlock(DFSOutputStream.java:705)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:894)
23/09/20 00:56:40 INFO db.DBInputFormat: Using read commited transaction isolation
23/09/20 00:56:40 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(`Unique_City_ID`), MAX(`Unique_City_ID`) FROM `cities_crosswalk`
23/09/20 00:56:40 WARN db.TextSplitter: Generating splits for a textual index column.
23/09/20 00:56:40 WARN db.TextSplitter: If your database sorts in a case-insensitive order, this may result in a partial import or duplicate records.
23/09/20 00:56:40 WARN db.TextSplitter: You are strongly encouraged to choose an integral split column.
23/09/20 00:56:40 WARN hdfs.DFSClient: Caught exception 
java.lang.InterruptedException
	at java.lang.Object.wait(Native Method)
	at java.lang.Thread.join(Thread.java:1281)
	at java.lang.Thread.join(Thread.java:1355)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFSOutputStream.java:967)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.endBlock(DFSOutputStream.java:705)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:894)
23/09/20 00:56:40 INFO mapreduce.JobSubmitter: number of splits:6
23/09/20 00:56:41 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1695115881461_0002
23/09/20 00:56:42 INFO impl.YarnClientImpl: Submitted application application_1695115881461_0002
23/09/20 00:56:42 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1695115881461_0002/
23/09/20 00:56:42 INFO mapreduce.Job: Running job: job_1695115881461_0002
23/09/20 00:57:01 INFO mapreduce.Job: Job job_1695115881461_0002 running in uber mode : false
23/09/20 00:57:01 INFO mapreduce.Job:  map 0% reduce 0%
23/09/20 00:58:28 INFO mapreduce.Job:  map 17% reduce 0%
23/09/20 00:58:36 INFO mapreduce.Job:  map 33% reduce 0%
23/09/20 00:58:37 INFO mapreduce.Job:  map 50% reduce 0%
23/09/20 00:58:38 INFO mapreduce.Job:  map 83% reduce 0%
23/09/20 00:58:39 INFO mapreduce.Job:  map 100% reduce 0%
23/09/20 00:58:40 INFO mapreduce.Job: Job job_1695115881461_0002 completed successfully
23/09/20 00:58:41 INFO mapreduce.Job: Counters: 31
	File System Counters
		FILE: Number of bytes read=0
		FILE: Number of bytes written=1028064
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=939
		HDFS: Number of bytes written=1074275
		HDFS: Number of read operations=24
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=12
	Job Counters 
		Killed map tasks=1
		Launched map tasks=7
		Other local map tasks=7
		Total time spent by all maps in occupied slots (ms)=538606
		Total time spent by all reduces in occupied slots (ms)=0
		Total time spent by all map tasks (ms)=538606
		Total vcore-milliseconds taken by all map tasks=538606
		Total megabyte-milliseconds taken by all map tasks=551532544
	Map-Reduce Framework
		Map input records=25341
		Map output records=25341
		Input split bytes=939
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=6735
		CPU time spent (ms)=14550
		Physical memory (bytes) snapshot=714596352
		Virtual memory (bytes) snapshot=9065861120
		Total committed heap usage (bytes)=364511232
	File Input Format Counters 
		Bytes Read=0
	File Output Format Counters 
		Bytes Written=1074275
23/09/20 00:58:41 INFO mapreduce.ImportJobBase: Transferred 1.0245 MB in 127.522 seconds (8.2268 KB/sec)
23/09/20 00:58:41 INFO mapreduce.ImportJobBase: Retrieved 25341 records.
[cloudera@quickstart ~]$ 

