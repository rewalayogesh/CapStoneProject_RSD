am.java:894)
23/09/20 03:51:11 WARN hdfs.DFSClient: Caught exception 
java.lang.InterruptedException
	at java.lang.Object.wait(Native Method)
	at java.lang.Thread.join(Thread.java:1281)
	at java.lang.Thread.join(Thread.java:1355)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFSOutputStream.java:967)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.endBlock(DFSOutputStream.java:705)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:894)
23/09/20 03:51:11 WARN hdfs.DFSClient: Caught exception 
java.lang.InterruptedException
	at java.lang.Object.wait(Native Method)
	at java.lang.Thread.join(Thread.java:1281)
	at java.lang.Thread.join(Thread.java:1355)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFSOutputStream.java:967)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.endBlock(DFSOutputStream.java:705)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:894)
23/09/20 03:51:11 WARN hdfs.DFSClient: Caught exception 
java.lang.InterruptedException
	at java.lang.Object.wait(Native Method)
	at java.lang.Thread.join(Thread.java:1281)
	at java.lang.Thread.join(Thread.java:1355)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFSOutputStream.java:967)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.endBlock(DFSOutputStream.java:705)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:894)
23/09/20 03:51:12 WARN hdfs.DFSClient: Caught exception 
java.lang.InterruptedException
	at java.lang.Object.wait(Native Method)
	at java.lang.Thread.join(Thread.java:1281)
	at java.lang.Thread.join(Thread.java:1355)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFSOutputStream.java:967)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.endBlock(DFSOutputStream.java:705)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:894)
23/09/20 03:51:12 WARN hdfs.DFSClient: Caught exception 
java.lang.InterruptedException
	at java.lang.Object.wait(Native Method)
	at java.lang.Thread.join(Thread.java:1281)
	at java.lang.Thread.join(Thread.java:1355)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFSOutputStream.java:967)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.endBlock(DFSOutputStream.java:705)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:894)
23/09/20 03:51:12 WARN hdfs.DFSClient: Caught exception 
java.lang.InterruptedException
	at java.lang.Object.wait(Native Method)
	at java.lang.Thread.join(Thread.java:1281)
	at java.lang.Thread.join(Thread.java:1355)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFSOutputStream.java:967)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeInternal(DFSOutputStream.java:935)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:931)
23/09/20 03:51:12 WARN hdfs.DFSClient: Caught exception 
java.lang.InterruptedException
	at java.lang.Object.wait(Native Method)
	at java.lang.Thread.join(Thread.java:1281)
	at java.lang.Thread.join(Thread.java:1355)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFSOutputStream.java:967)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.endBlock(DFSOutputStream.java:705)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:894)
23/09/20 03:51:12 WARN hdfs.DFSClient: Caught exception 
java.lang.InterruptedException
	at java.lang.Object.wait(Native Method)
	at java.lang.Thread.join(Thread.java:1281)
	at java.lang.Thread.join(Thread.java:1355)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFSOutputStream.java:967)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.endBlock(DFSOutputStream.java:705)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:894)
23/09/20 03:51:13 WARN hdfs.DFSClient: Caught exception 
java.lang.InterruptedException
	at java.lang.Object.wait(Native Method)
	at java.lang.Thread.join(Thread.java:1281)
	at java.lang.Thread.join(Thread.java:1355)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFSOutputStream.java:967)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.endBlock(DFSOutputStream.java:705)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:894)
23/09/20 03:51:13 INFO db.DBInputFormat: Using read commited transaction isolation
23/09/20 03:51:13 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(`Date`), MAX(`Date`) FROM `City_data`
23/09/20 03:51:13 INFO db.IntegerSplitter: Split size: 55900800000; Num splits: 4 from: 1291104000000 to: 1514707200000
23/09/20 03:51:14 WARN hdfs.DFSClient: Caught exception 
java.lang.InterruptedException
	at java.lang.Object.wait(Native Method)
	at java.lang.Thread.join(Thread.java:1281)
	at java.lang.Thread.join(Thread.java:1355)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFSOutputStream.java:967)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.endBlock(DFSOutputStream.java:705)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:894)
23/09/20 03:51:14 WARN hdfs.DFSClient: Caught exception 
java.lang.InterruptedException
	at java.lang.Object.wait(Native Method)
	at java.lang.Thread.join(Thread.java:1281)
	at java.lang.Thread.join(Thread.java:1355)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFSOutputStream.java:967)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.endBlock(DFSOutputStream.java:705)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:894)
23/09/20 03:51:14 INFO mapreduce.JobSubmitter: number of splits:4
23/09/20 03:51:14 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1695115881461_0003
23/09/20 03:51:15 INFO impl.YarnClientImpl: Submitted application application_1695115881461_0003
23/09/20 03:51:15 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1695115881461_0003/
23/09/20 03:51:15 INFO mapreduce.Job: Running job: job_1695115881461_0003
23/09/20 03:51:38 INFO mapreduce.Job: Job job_1695115881461_0003 running in uber mode : false
23/09/20 03:51:38 INFO mapreduce.Job:  map 0% reduce 0%
23/09/20 03:53:09 INFO mapreduce.Job:  map 25% reduce 0%
23/09/20 03:53:13 INFO mapreduce.Job:  map 50% reduce 0%
23/09/20 03:53:15 INFO mapreduce.Job:  map 100% reduce 0%
23/09/20 03:53:16 INFO mapreduce.Job: Job job_1695115881461_0003 completed successfully
23/09/20 03:53:16 INFO mapreduce.Job: Counters: 31
	File System Counters
		FILE: Number of bytes read=0
		FILE: Number of bytes written=685308
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=497
		HDFS: Number of bytes written=43187049
		HDFS: Number of read operations=16
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=8
	Job Counters 
		Killed map tasks=1
		Launched map tasks=5
		Other local map tasks=5
		Total time spent by all maps in occupied slots (ms)=368966
		Total time spent by all reduces in occupied slots (ms)=0
		Total time spent by all map tasks (ms)=368966
		Total vcore-milliseconds taken by all map tasks=368966
		Total megabyte-milliseconds taken by all map tasks=377821184
	Map-Reduce Framework
		Map input records=919776
		Map output records=919776
		Input split bytes=497
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=2649
		CPU time spent (ms)=28960
		Physical memory (bytes) snapshot=492945408
		Virtual memory (bytes) snapshot=6045593600
		Total committed heap usage (bytes)=243007488
	File Input Format Counters 
		Bytes Read=0
	File Output Format Counters 
		Bytes Written=43187049
23/09/20 03:53:16 INFO mapreduce.ImportJobBase: Transferred 41.1864 MB in 129.1324 seconds (326.6015 KB/sec)
23/09/20 03:53:16 INFO mapreduce.ImportJobBase: Retrieved 919776 records.
23/09/20 03:53:17 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `City_data` AS t LIMIT 1
23/09/20 03:53:17 WARN hive.TableDefWriter: Column Date had to be cast to a less precise type in Hive
23/09/20 03:53:17 INFO hive.HiveImport: Loading uploaded data into Hive

Logging initialized using configuration in jar:file:/usr/lib/hive/lib/hive-common-1.1.0-cdh5.13.0.jar!/hive-log4j.properties
OK
Time taken: 5.463 seconds
Loading data to table capstone.city_data
Table capstone.city_data stats: [numFiles=4, totalSize=43187049]
OK
Time taken: 2.221 seconds
[cloudera@quickstart ~]$ sqoop import \
> --connect jdbc:mysql://localhost/Capstone \
> --username=root --password=cloudera \
> --table cities_crosswalk \
> --hive-import --hive-table Capstone.cities_crosswalk
Warning: /usr/lib/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.
23/09/20 03:58:21 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.13.0
23/09/20 03:58:21 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.
23/09/20 03:58:21 INFO tool.BaseSqoopTool: Using Hive-specific delimiters for output. You can override
23/09/20 03:58:21 INFO tool.BaseSqoopTool: delimiters with --fields-terminated-by, etc.
23/09/20 03:58:22 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.
23/09/20 03:58:22 INFO tool.CodeGenTool: Beginning code generation
23/09/20 03:58:24 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `cities_crosswalk` AS t LIMIT 1
23/09/20 03:58:24 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `cities_crosswalk` AS t LIMIT 1
23/09/20 03:58:24 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /usr/lib/hadoop-mapreduce
Note: /tmp/sqoop-cloudera/compile/be19058bf022aa53344e2af7d6f3207c/cities_crosswalk.java uses or overrides a deprecated API.
Note: Recompile with -Xlint:deprecation for details.
23/09/20 03:58:32 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-cloudera/compile/be19058bf022aa53344e2af7d6f3207c/cities_crosswalk.jar
23/09/20 03:58:32 WARN manager.MySQLManager: It looks like you are importing from mysql.
23/09/20 03:58:32 WARN manager.MySQLManager: This transfer can be faster! Use the --direct
23/09/20 03:58:32 WARN manager.MySQLManager: option to exercise a MySQL-specific fast path.
23/09/20 03:58:32 INFO manager.MySQLManager: Setting zero DATETIME behavior to convertToNull (mysql)
23/09/20 03:58:32 INFO mapreduce.ImportJobBase: Beginning import of cities_crosswalk
23/09/20 03:58:32 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address
23/09/20 03:58:33 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar
23/09/20 03:58:37 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps
23/09/20 03:58:37 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032
23/09/20 03:58:40 WARN hdfs.DFSClient: Caught exception 
java.lang.InterruptedException
	at java.lang.Object.wait(Native Method)
	at java.lang.Thread.join(Thread.java:1281)
	at java.lang.Thread.join(Thread.java:1355)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFSOutputStream.java:967)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.endBlock(DFSOutputStream.java:705)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:894)
23/09/20 03:58:40 WARN hdfs.DFSClient: Caught exception 
java.lang.InterruptedException
	at java.lang.Object.wait(Native Method)
	at java.lang.Thread.join(Thread.java:1281)
	at java.lang.Thread.join(Thread.java:1355)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFSOutputStream.java:967)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.endBlock(DFSOutputStream.java:705)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:894)
23/09/20 03:58:40 WARN hdfs.DFSClient: Caught exception 
java.lang.InterruptedException
	at java.lang.Object.wait(Native Method)
	at java.lang.Thread.join(Thread.java:1281)
	at java.lang.Thread.join(Thread.java:1355)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFSOutputStream.java:967)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.endBlock(DFSOutputStream.java:705)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:894)
23/09/20 03:58:40 WARN hdfs.DFSClient: Caught exception 
java.lang.InterruptedException
	at java.lang.Object.wait(Native Method)
	at java.lang.Thread.join(Thread.java:1281)
	at java.lang.Thread.join(Thread.java:1355)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFSOutputStream.java:967)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.endBlock(DFSOutputStream.java:705)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:894)
23/09/20 03:58:40 WARN hdfs.DFSClient: Caught exception 
java.lang.InterruptedException
	at java.lang.Object.wait(Native Method)
	at java.lang.Thread.join(Thread.java:1281)
	at java.lang.Thread.join(Thread.java:1355)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFSOutputStream.java:967)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.endBlock(DFSOutputStream.java:705)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:894)
23/09/20 03:58:40 WARN hdfs.DFSClient: Caught exception 
java.lang.InterruptedException
	at java.lang.Object.wait(Native Method)
	at java.lang.Thread.join(Thread.java:1281)
	at java.lang.Thread.join(Thread.java:1355)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFSOutputStream.java:967)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.endBlock(DFSOutputStream.java:705)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:894)
23/09/20 03:58:41 WARN hdfs.DFSClient: Caught exception 
java.lang.InterruptedException
	at java.lang.Object.wait(Native Method)
	at java.lang.Thread.join(Thread.java:1281)
	at java.lang.Thread.join(Thread.java:1355)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFSOutputStream.java:967)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.endBlock(DFSOutputStream.java:705)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:894)
23/09/20 03:58:41 WARN hdfs.DFSClient: Caught exception 
java.lang.InterruptedException
	at java.lang.Object.wait(Native Method)
	at java.lang.Thread.join(Thread.java:1281)
	at java.lang.Thread.join(Thread.java:1355)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFSOutputStream.java:967)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.endBlock(DFSOutputStream.java:705)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:894)
23/09/20 03:58:41 WARN hdfs.DFSClient: Caught exception 
java.lang.InterruptedException
	at java.lang.Object.wait(Native Method)
	at java.lang.Thread.join(Thread.java:1281)
	at java.lang.Thread.join(Thread.java:1355)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFSOutputStream.java:967)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.endBlock(DFSOutputStream.java:705)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:894)
23/09/20 03:58:41 WARN hdfs.DFSClient: Caught exception 
java.lang.InterruptedException
	at java.lang.Object.wait(Native Method)
	at java.lang.Thread.join(Thread.java:1281)
	at java.lang.Thread.join(Thread.java:1355)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFSOutputStream.java:967)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.endBlock(DFSOutputStream.java:705)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:894)
23/09/20 03:58:41 WARN hdfs.DFSClient: Caught exception 
java.lang.InterruptedException
	at java.lang.Object.wait(Native Method)
	at java.lang.Thread.join(Thread.java:1281)
	at java.lang.Thread.join(Thread.java:1355)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFSOutputStream.java:967)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.endBlock(DFSOutputStream.java:705)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:894)
23/09/20 03:58:41 WARN hdfs.DFSClient: Caught exception 
java.lang.InterruptedException
	at java.lang.Object.wait(Native Method)
	at java.lang.Thread.join(Thread.java:1281)
	at java.lang.Thread.join(Thread.java:1355)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFSOutputStream.java:967)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.endBlock(DFSOutputStream.java:705)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:894)
23/09/20 03:58:41 WARN hdfs.DFSClient: Caught exception 
java.lang.InterruptedException
	at java.lang.Object.wait(Native Method)
	at java.lang.Thread.join(Thread.java:1281)
	at java.lang.Thread.join(Thread.java:1355)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFSOutputStream.java:967)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.endBlock(DFSOutputStream.java:705)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:894)
23/09/20 03:58:42 WARN hdfs.DFSClient: Caught exception 
java.lang.InterruptedException
	at java.lang.Object.wait(Native Method)
	at java.lang.Thread.join(Thread.java:1281)
	at java.lang.Thread.join(Thread.java:1355)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFSOutputStream.java:967)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.endBlock(DFSOutputStream.java:705)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:894)
23/09/20 03:58:42 WARN hdfs.DFSClient: Caught exception 
java.lang.InterruptedException
	at java.lang.Object.wait(Native Method)
	at java.lang.Thread.join(Thread.java:1281)
	at java.lang.Thread.join(Thread.java:1355)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFSOutputStream.java:967)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.endBlock(DFSOutputStream.java:705)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:894)
23/09/20 03:58:42 INFO db.DBInputFormat: Using read commited transaction isolation
23/09/20 03:58:42 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(`Unique_City_ID`), MAX(`Unique_City_ID`) FROM `cities_crosswalk`
23/09/20 03:58:42 WARN db.TextSplitter: Generating splits for a textual index column.
23/09/20 03:58:42 WARN db.TextSplitter: If your database sorts in a case-insensitive order, this may result in a partial import or duplicate records.
23/09/20 03:58:42 WARN db.TextSplitter: You are strongly encouraged to choose an integral split column.
23/09/20 03:58:43 WARN hdfs.DFSClient: Caught exception 
java.lang.InterruptedException
	at java.lang.Object.wait(Native Method)
	at java.lang.Thread.join(Thread.java:1281)
	at java.lang.Thread.join(Thread.java:1355)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFSOutputStream.java:967)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.endBlock(DFSOutputStream.java:705)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:894)
23/09/20 03:58:43 WARN hdfs.DFSClient: Caught exception 
java.lang.InterruptedException
	at java.lang.Object.wait(Native Method)
	at java.lang.Thread.join(Thread.java:1281)
	at java.lang.Thread.join(Thread.java:1355)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFSOutputStream.java:967)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.endBlock(DFSOutputStream.java:705)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:894)
23/09/20 03:58:43 INFO mapreduce.JobSubmitter: number of splits:6
23/09/20 03:58:43 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1695115881461_0004
23/09/20 03:58:44 INFO impl.YarnClientImpl: Submitted application application_1695115881461_0004
23/09/20 03:58:44 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1695115881461_0004/
23/09/20 03:58:44 INFO mapreduce.Job: Running job: job_1695115881461_0004
23/09/20 03:59:11 INFO mapreduce.Job: Job job_1695115881461_0004 running in uber mode : false
23/09/20 03:59:11 INFO mapreduce.Job:  map 0% reduce 0%
23/09/20 04:00:27 INFO mapreduce.Job:  map 17% reduce 0%
23/09/20 04:00:34 INFO mapreduce.Job:  map 50% reduce 0%
23/09/20 04:00:36 INFO mapreduce.Job:  map 67% reduce 0%
23/09/20 04:01:44 INFO mapreduce.Job:  map 83% reduce 0%
23/09/20 04:01:46 INFO mapreduce.Job:  map 100% reduce 0%
23/09/20 04:01:49 INFO mapreduce.Job: Job job_1695115881461_0004 completed successfully
23/09/20 04:01:50 INFO mapreduce.Job: Counters: 31
	File System Counters
		FILE: Number of bytes read=0
		FILE: Number of bytes written=1028220
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=939
		HDFS: Number of bytes written=1074275
		HDFS: Number of read operations=24
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=12
	Job Counters 
		Killed map tasks=3
		Launched map tasks=7
		Other local map tasks=7
		Total time spent by all maps in occupied slots (ms)=505440
		Total time spent by all reduces in occupied slots (ms)=0
		Total time spent by all map tasks (ms)=505440
		Total vcore-milliseconds taken by all map tasks=505440
		Total megabyte-milliseconds taken by all map tasks=517570560
	Map-Reduce Framework
		Map input records=25341
		Map output records=25341
		Input split bytes=939
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=4698
		CPU time spent (ms)=15580
		Physical memory (bytes) snapshot=702357504
		Virtual memory (bytes) snapshot=9067950080
		Total committed heap usage (bytes)=364511232
	File Input Format Counters 
		Bytes Read=0
	File Output Format Counters 
		Bytes Written=1074275
23/09/20 04:01:50 INFO mapreduce.ImportJobBase: Transferred 1.0245 MB in 193.7339 seconds (5.4151 KB/sec)
23/09/20 04:01:50 INFO mapreduce.ImportJobBase: Retrieved 25341 records.
23/09/20 04:01:51 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `cities_crosswalk` AS t LIMIT 1
23/09/20 04:01:51 INFO hive.HiveImport: Loading uploaded data into Hive

Logging initialized using configuration in jar:file:/usr/lib/hive/lib/hive-common-1.1.0-cdh5.13.0.jar!/hive-log4j.properties
OK
Time taken: 11.668 seconds
Loading data to table capstone.cities_crosswalk
Table capstone.cities_crosswalk stats: [numFiles=6, totalSize=1074275]
OK
Time taken: 2.899 seconds
[cloudera@quickstart ~]$ 

